ok so far i've tested autoregressive models with no vector quantization

i suspect having a continuous output is also probably bad because as the image generates the values may drift
what if i try having 256 different possible outputs from the neural network and then use cross entropy


7/6/24 
even over training wasn't working but now it is. i think its because i wasn't batching and the learning rate was too high
now i am training over all images and the loss is plateauing again. i think i should switch to convolutional layers to scale up better


6/30/24
i am now classifying between 256 possible grayscale pixel values
the loss is not dropping very far though
i think i should look into how distribution of data can affect outcome
i should first see if i can overtrain on a very small part of the data



